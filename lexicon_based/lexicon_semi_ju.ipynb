{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Semi-supervised Induction of Affect Lexicons\n",
    "## 预处理\n",
    "引入包，还有下载nltk包"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# ————\n",
    "# coding=utf-8\n",
    "import os\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.stem\n",
    "import string\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "# extract the review to train on word2vec\n",
    "# and I find word2vec only receive list of lists\n",
    "# https://towardsdatascience.com/a-beginners-guide-to-word-embedding-with-gensim-word2vec-model-5970fa56cc92\n",
    "# nltk.download('punkt')   english word tokenize\n",
    "# nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "读取json文件，并且进行预处理，并保存"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['handcream', 'beautiful', 'fragrance', 'doesnt', 'stay', 'protect', 'hands', 'washing', 'size', 'quite', 'small'], ['wonderful', 'hand', 'lotion', 'seriously', 'dry', 'skin', 'stays', 'long', 'time', 'little', 'goes', 'long', 'long', 'way', 'go', 'easy', 'wonderful', 'scent', 'maybe', 'bit', 'strong', 'first', 'dissipates'], ['best', 'hand', 'cream', 'around', 'silky', 'thick', 'soaks', 'way', 'leaving', 'hands', 'super', 'soft'], ['thanks'], ['great', 'hand', 'lotion', 'soaks', 'right', 'leaves', 'skin', 'super', 'soft', 'greasy', 'residue', 'great', 'scent']]\n"
     ]
    }
   ],
   "source": [
    "# read the corpus json file\n",
    "# do preprocessing\n",
    "fileName = 'Luxury_Beauty_5.json'\n",
    "# jsonPath = '../dataset/Amazon/' + fileName\n",
    "jsonPath = './NLP/SentimentAnalysis_2021_6/dataset/Amazon/' + fileName\n",
    "\n",
    "text_processed = []\n",
    "o = open(jsonPath, 'r')\n",
    "for line_json in open(jsonPath, 'r'):\n",
    "    line_review = json.loads(line_json)\n",
    "    if 'reviewText' in line_review.keys():  # to avoid json without review\n",
    "        line_review = line_review['reviewText']\n",
    "        # preprocessing  https://blog.csdn.net/weixin_43216017/article/details/88324093\n",
    "        # transfer to lower case\n",
    "        line_review_lower = line_review.lower()\n",
    "        # remove the punctuation\n",
    "        remove = str.maketrans('', '', string.punctuation)\n",
    "        without_punctuation = line_review_lower.translate(remove)\n",
    "        # tokenize\n",
    "        tokens = nltk.word_tokenize(without_punctuation)\n",
    "        # remove the word not used\n",
    "        without_stopwords = [w for w in tokens if w not in stopwords.words('english')]\n",
    "        # to extract the stem  ## I find this will transfer nothing to noth\n",
    "        # s = nltk.stem.SnowballStemmer('english')  # para is the chosen language\n",
    "        # cleaned_text = [s.stem(ws) for ws in without_stopwords]\n",
    "        text_processed.append(without_stopwords)\n",
    "# print(text_processed)\n",
    "print(text_processed[:5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "保存训练集——一个嵌套的list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# 保存text_processed，即预处理后的嵌套列表\n",
    "text_processed = np.array(text_processed, dtype=list)\n",
    "np.save('./NLP/SentimentAnalysis_2021_6/dataset/Amazon/result/lexicon_semi/text_processed.npy', text_processed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. 训练模型 并进行存储\n",
    "，之前非常疑惑输入进word2vec的sentences是什么，现在由\n",
    "[博客](https://towardsdatascience.com/a-beginners-guide-to-word-embedding-with-gensim-word2vec-model-5970fa56cc92)\n",
    "可以知道，输入进入word2vec的sentences就是嵌套的`list`\n",
    "2. 存储word2vec，以及使用sentence generator（当数据过大的时候，需要使用`yield`来慢慢生成sentence）\n",
    "参考 [博客](https://blog.csdn.net/u010665216/article/details/78709018)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydev_jupyter_utils'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-2-2f63f773c2e4>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0msys\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'D:\\\\Tool\\\\Software\\\\JetBrain\\\\Pycharm\\\\PyCharm 2020.1.2\\\\plugins\\\\python\\\\helpers\\\\pydev'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0msys\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'D:\\\\Tool\\\\Software\\\\JetBrain\\\\Pycharm\\\\PyCharm 2020.1.2\\\\plugins\\\\python\\\\helpers-pro\\\\jupyter_debug'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mpydev_jupyter_utils\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m \u001B[0mpydev_jupyter_utils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mattach_to_debugger\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m54438\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'pydev_jupyter_utils'"
     ]
    }
   ],
   "source": [
    "# ————\n",
    "# 加载预处理后的数据\n",
    "text_processed = np.load('./NLP/SentimentAnalysis_2021_6/dataset/Amazon/result/lexicon_semi/text_processed.npy',\n",
    "                         allow_pickle=True).tolist()\n",
    "print(text_processed[:5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# 训练 Word2Vec 模型\n",
    "model = Word2Vec(text_processed, min_count=1, vector_size=50, workers=3, window=3, sg=1)\n",
    "# 在这里存储训练的模型\n",
    "model.save('./NLP/SentimentAnalysis_2021_6/dataset/Amazon/model/' + 'word2vec_model')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "从txt文件中生成 seed words 的list，\n",
    "并且得到 all_words，相当于我们词典的索引，待会儿将对每一个单词赋值一个词向量"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# ————\n",
    "# generate the list of seed words\n",
    "wordsTxtFileName0 = 'positive-words.txt'\n",
    "wordsTxtFileName1 = 'negative-words.txt'\n",
    "wordsTxtFileDir = './NLP/SentimentAnalysis_2021_6/dataset/Amazon/'\n",
    "f0 = open((wordsTxtFileDir + wordsTxtFileName0), 'r')\n",
    "f1 = open((wordsTxtFileDir + wordsTxtFileName1), 'r')\n",
    "remove_lf = str.maketrans('', '', '\\n')\n",
    "positive_list = [x.translate(remove_lf) for x in f0]\n",
    "negative_list = [x.translate(remove_lf) for x in f1]\n",
    "\n",
    "# generate the list contain all the words\n",
    "all_words = []\n",
    "for t in text_processed:\n",
    "    all_words.extend(t)\n",
    "all_words = list(set(all_words))  # to remove duplicate words\n",
    "all_set_words = set(all_words)  # use set to make it quicker to judge whether a word is in the corpus"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "生成seed words 的词向量"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# ————\n",
    "# 先加载模型,这样子下面的两个methods就不需要加载模型了\n",
    "model = gensim.models.Word2Vec.load('./NLP/SentimentAnalysis_2021_6/dataset/Amazon/model/' + 'word2vec_model')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "po_list_exist:['abound', 'abundance', 'abundant', 'accessible', 'acclaimed']\n",
      "ne_list_exist:['abnormal', 'abrade', 'abrasive', 'abrupt', 'absence']\n",
      "po_vec_exist:[array([-0.02088266, -0.05804365, -0.13800192, -0.09055097,  0.02085192,\n",
      "       -0.0149138 ,  0.12242734,  0.17210057, -0.1859646 , -0.1932898 ,\n",
      "       -0.17637585, -0.15095703,  0.00278641,  0.09003559, -0.03674203,\n",
      "       -0.06391171, -0.00361412, -0.07753745, -0.20702982, -0.10908903,\n",
      "       -0.11125408,  0.03487507,  0.10685436, -0.05433651, -0.0182542 ,\n",
      "        0.0037927 , -0.07509276, -0.3124628 , -0.15796928,  0.22880356,\n",
      "        0.2129787 ,  0.0614556 , -0.11976278,  0.0724825 , -0.01971441,\n",
      "        0.02461305, -0.163662  ,  0.27534336, -0.04038653,  0.01992375,\n",
      "        0.10122304, -0.11983544, -0.1891549 , -0.00068976,  0.12558703,\n",
      "        0.1126511 , -0.09438882, -0.03307529,  0.01192508,  0.21955687],\n",
      "      dtype=float32), array([-0.15776505, -0.12909171, -0.05213962, -0.15851481,  0.09765358,\n",
      "       -0.08411869,  0.24671093,  0.28828406, -0.2360858 , -0.23601525,\n",
      "       -0.01056583, -0.2623131 ,  0.13628803,  0.00564637, -0.21449322,\n",
      "       -0.15424323,  0.01505578,  0.04757867, -0.2556642 , -0.14017378,\n",
      "        0.02088111,  0.05799315,  0.28283653, -0.13867183, -0.07110434,\n",
      "        0.01231998, -0.06234158, -0.26091656, -0.27311215,  0.27146497,\n",
      "        0.16496836,  0.00629353, -0.05962088,  0.13112561,  0.0137101 ,\n",
      "        0.05893961, -0.16677867,  0.26437125, -0.0904218 , -0.0838304 ,\n",
      "        0.11804416, -0.05014465, -0.2110853 ,  0.06503947,  0.29223245,\n",
      "        0.06187927, -0.14154707, -0.12603933,  0.17832051,  0.25713655],\n",
      "      dtype=float32)]\n",
      "ne_vec_exist:[array([-0.10729925,  0.00703874,  0.05336671,  0.02203897,  0.11356238,\n",
      "        0.04257661,  0.19996831,  0.35012358, -0.20519157, -0.3473904 ,\n",
      "       -0.0115965 , -0.2838856 ,  0.12071095,  0.07164419, -0.26331162,\n",
      "       -0.105299  , -0.03936331, -0.00981115, -0.36244145, -0.1865642 ,\n",
      "        0.07417414,  0.14086385,  0.23245741, -0.17479841, -0.0198298 ,\n",
      "        0.04235393, -0.10693161, -0.22619963, -0.0932139 ,  0.18853223,\n",
      "       -0.04102008, -0.0182151 , -0.10351811,  0.04447408, -0.02745558,\n",
      "        0.064357  , -0.17135456,  0.23924239, -0.10039905, -0.04435384,\n",
      "        0.27578333, -0.07294846, -0.12325912,  0.09811761,  0.2927259 ,\n",
      "        0.217598  ,  0.02887555, -0.2134715 ,  0.06855259,  0.10813625],\n",
      "      dtype=float32), array([-0.03478388, -0.01043825, -0.01560163, -0.00177325,  0.0420114 ,\n",
      "        0.00154541,  0.04099264,  0.06516509, -0.06692599, -0.08305068,\n",
      "        0.00080741, -0.08673387,  0.01610757,  0.03934274, -0.08497925,\n",
      "       -0.04157203,  0.01781204, -0.03990993, -0.05603087, -0.01219087,\n",
      "        0.01364529,  0.02598551,  0.04438686, -0.01923591, -0.01528168,\n",
      "       -0.0026692 , -0.03566075, -0.07136767, -0.06311968,  0.09654456,\n",
      "        0.0405866 , -0.00234889, -0.03032465,  0.01231411, -0.00206232,\n",
      "        0.04278854, -0.0341874 ,  0.04449435, -0.03426021, -0.0066362 ,\n",
      "        0.03365798,  0.01620197, -0.0732057 ,  0.01738379,  0.02945028,\n",
      "        0.06435607, -0.05894727,  0.01148096,  0.04267716,  0.07730159],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# ————\n",
    "# generate the vectors of seed words\n",
    "po_list_exist = []\n",
    "ne_list_exist = []\n",
    "po_vec_exist = []\n",
    "ne_vec_exist = []\n",
    "for w in positive_list:\n",
    "    if w in all_words:\n",
    "        po_list_exist.append(w)\n",
    "        po_vec_exist.append(model.wv[w])\n",
    "for w in negative_list:\n",
    "    if w in all_words:\n",
    "        ne_list_exist.append(w)\n",
    "        ne_vec_exist.append(model.wv[w])\n",
    "print('po_list_exist:' + str(po_list_exist[:5]) + '\\n' + 'ne_list_exist:'\n",
    "      + str(ne_list_exist[:5]) + '\\n' + 'po_vec_exist:' + str(po_vec_exist[:2])\n",
    "      + '\\n' + 'ne_vec_exist:' + str(ne_vec_exist[:2]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Semantic Axis Methods\n",
    "使用axis methods并且存储得到的词典"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "# Semantic Axis Methods\n",
    "# start calculation on the paper\n",
    "V_plus = np.zeros(po_vec_exist[0].shape, dtype=float)\n",
    "V_minus = np.zeros(ne_vec_exist[0].shape, dtype=float)\n",
    "for e_w_i in po_vec_exist:\n",
    "    V_plus = V_plus + e_w_i\n",
    "V_plus = V_plus / len(po_vec_exist)\n",
    "for e_w_i in ne_vec_exist:\n",
    "    V_minus = V_minus + e_w_i\n",
    "V_minus = V_minus / len(ne_vec_exist)\n",
    "# calculate V_axis\n",
    "V_axis = V_plus - V_minus\n",
    "# calculate score of every word\n",
    "score_list_axis = []\n",
    "# 初始化字典\n",
    "lexicon_dic_axis = {}\n",
    "for w in all_words:\n",
    "    w_vec = model.wv[w]\n",
    "    cos_value = np.dot(w_vec, V_axis) / (np.linalg.norm(w_vec, ord=2) * np.linalg.norm(V_axis, ord=2))\n",
    "    score_list_axis.append(cos_value)\n",
    "    lexicon_dic_axis[w] = [cos_value]\n",
    "# print(score_list_axis)\n",
    "# 存储字典\n",
    "np.save('./NLP/SentimentAnalysis_2021_6/dataset/Amazon/result/lexicon_semi/lexicon_dic_axis.npy', lexicon_dic_axis)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Label Propagation\n",
    "计算 $E$ 矩阵"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Label Propagation\n",
    "# initialize an empty matrix\n",
    "n = len(all_words)\n",
    "E = np.zeros([n, n], dtype=float)  # define the edge\n",
    "for i in range(0, n):\n",
    "    w_vec_i = model.wv[all_words[i]]\n",
    "    for j in range(0, n):\n",
    "        w_vec_j = model.wv[all_words[j]]\n",
    "        v = (-1) * np.dot(w_vec_i, w_vec_j) / (np.linalg.norm(w_vec_i, ord=2) * np.linalg.norm(w_vec_j, ord=2))\n",
    "        # 为了解决精度问题\n",
    "        if v > 1:\n",
    "            v = 1\n",
    "        elif v < -1:\n",
    "            v = -1\n",
    "        E[i][j] = np.arccos(v)\n",
    "# 参考维基百科 page rank，里面描述了矩阵的意义 l(p_i,p_j)是 从页面j->i的链接数/页面j中含有的外部链接总数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "初始化 $p$ 和 $D$ 并计算 $D$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 初始化p 为 1/n\n",
    "p = np.array([1 / n] * n, dtype=float)  # 创建一个有n个元素都是1/n的list，然后再转换成 numpy\n",
    "D = np.zeros([n, n], dtype=float)  # 初始化D\n",
    "# 计算D\n",
    "column_sum = E.sum(axis=0)\n",
    "for i in range(0, n):\n",
    "    D[i][i] = column_sum[i]\n",
    "D_1_divide_2 = np.sqrt(D)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "计算 $T$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 计算T\n",
    "T = D_1_divide_2 * E * D_1_divide_2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "计算 $s$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 计算s\n",
    "s_po = np.zeros([n], dtype=float)\n",
    "n_po = len(po_list_exist)\n",
    "s_ne = np.zeros([n], dtype=float)\n",
    "n_ne = len(ne_list_exist)\n",
    "for i in range(0, n):\n",
    "    if all_words[i] in po_list_exist:\n",
    "        s_po[i] = 1 / n_po\n",
    "    if all_words[i] in ne_list_exist:\n",
    "        s_ne[i] = 1 / n_ne"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "设置基尼系数beta  来自于[维基百科](https://zh.wikipedia.org/zh-cn/PageRank)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "beta = 0.85"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "开始迭代"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 开始迭代   TODO: 无法确认要迭代多少次\n",
    "p_po = p\n",
    "p_ne = p\n",
    "for i in range(0, 50):\n",
    "    p_po = beta * np.dot(T, p_po) + (1 - beta) * s_po\n",
    "    p_ne = beta * np.dot(T, p_ne) + (1 - beta) * s_ne"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "计算每一个单词的score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 计算每一个单词的score\n",
    "score_plus_propagation = np.zeros([n], dtype=float)\n",
    "score_minus_propagation = np.zeros([n], dtype=float)\n",
    "# 初始化字典\n",
    "lexicon_dic_propagation = {}\n",
    "for i in range(0, n):\n",
    "    score_plus_propagation[i] = p_po[i] / (p_po[i] + p_ne[i])\n",
    "    score_minus_propagation[i] = p_ne[i] / (p_po[i] + p_ne[i])\n",
    "    # 装到字典里面，后面好取用\n",
    "    lexicon_dic_propagation[all_words[i]] = [score_plus_propagation[i], score_minus_propagation[i]]\n",
    "# 存储字典\n",
    "np.save('./NLP/SentimentAnalysis_2021_6/dataset/Amazon/result/lexicon_semi/lexicon_dic_propagation.npy',\n",
    "        lexicon_dic_propagation)\n",
    "# print(lexicon_dic_propagation)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using Lexicons for Sentiment Recognition\n",
    "看样子根据之前的 Semantic Axis Methods 和 Label Propagation 最后得到的结果来说，一个是 $score(w)$ 一个是 $score^+(w_i)$ 那应该意味着一个是要么为正要么为负，另一个是既有正值，也有负值"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 读取已每个词的value（上面已经算出了两种value了），然后加载两部词典\n",
    "lexicon_dic_axis = np.load('./NLP/SentimentAnalysis_2021_6/dataset/Amazon/result/lexicon_semi/lexicon_dic_axis.npy',\n",
    "                           allow_pickle=True).item()\n",
    "lexicon_dic_propagation = np.load(\n",
    "    './NLP/SentimentAnalysis_2021_6/dataset/Amazon/result/lexicon_semi/lexicon_dic_propagation.npy',\n",
    "    allow_pickle=True).item()\n",
    "# 加载要进行分析的语料\n",
    "fileName = 'Luxury_Beauty_5.json'\n",
    "jsonPath = './NLP/SentimentAnalysis_2021_6/dataset/Amazon/' + fileName\n",
    "line_reviews = []\n",
    "o = open(jsonPath, 'r')\n",
    "for line_json in o:\n",
    "    line_review = json.loads(line_json)\n",
    "    line_reviews.append(line_review)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 迭代并得出分别使用两部词典的情感分析"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 定义lambda值\n",
    "lam = 1\n",
    "# 用来存储句子的情感，either是第一种词典得到的 要么是正要么是负数的词典 ，both是第二种词典得到的 一个单词的权重有正有负\n",
    "# 用 1 来表示 positive     0 来表示 negative\n",
    "sentiment_recognition_either = []\n",
    "sentiment_recognition_both = []\n",
    "# as for Semantic Axis Methods. Either positive or negative\n",
    "positive_score = 0\n",
    "negative_score = 0\n",
    "# as for Label Propagation. Have both positive value or negative value\n",
    "f_plus = 0\n",
    "f_minus = 0\n",
    "\n",
    "# begin to iteration\n",
    "# 每句话先处理，再分别计算分\n",
    "for index, line in enumerate(line_reviews):\n",
    "    if 'reviewText' in line.keys():  # to avoid json without review\n",
    "        # TODO: 下面的代码可以包装成一个 preprocess(label,language) 来进行简化\n",
    "        line_review = line['reviewText']\n",
    "        # preprocessing  https://blog.csdn.net/weixin_43216017/article/details/88324093\n",
    "        # transfer to lower case\n",
    "        line_review_lower = line_review.lower()\n",
    "        # remove the punctuation\n",
    "        remove = str.maketrans('', '', string.punctuation)\n",
    "        without_punctuation = line_review_lower.translate(remove)\n",
    "        # tokenize\n",
    "        tokens = nltk.word_tokenize(without_punctuation)\n",
    "        # remove the word not used\n",
    "        without_stopwords = [w for w in tokens if w not in stopwords.words('english')]\n",
    "        # to extract the stem  ## I find this will transfer nothing to noth\n",
    "        # s = nltk.stem.SnowballStemmer('english')  # para is the chosen language\n",
    "        # cleaned_text = [s.stem(ws) for ws in without_stopwords]\n",
    "\n",
    "        # 开始计算值分析\n",
    "        for w in without_stopwords:\n",
    "            # —————————— axis ————————————\n",
    "            w_score_0 = lexicon_dic_axis[w][0]\n",
    "            if w_score_0 > 0:\n",
    "                positive_score += w_score_0\n",
    "            else:\n",
    "                # 注意，negative_score是小于0的\n",
    "                negative_score += w_score_0\n",
    "\n",
    "            # ——————————propagation ————————\n",
    "            w_score_1_0 = lexicon_dic_propagation[w][0]\n",
    "            w_score_1_1 = lexicon_dic_propagation[w][1]\n",
    "            f_plus += w_score_1_0\n",
    "            f_minus += w_score_1_1\n",
    "\n",
    "    # —————————— axis ————————————\n",
    "    if negative_score == 0:\n",
    "        sentiment_recognition_either.append(1)\n",
    "    elif positive_score == 0:\n",
    "        sentiment_recognition_either.append(0)\n",
    "    elif abs(positive_score / negative_score) > lam:\n",
    "        sentiment_recognition_either.append(1)\n",
    "    elif abs(negative_score / positive_score) > lam:\n",
    "        sentiment_recognition_either.append(0)\n",
    "\n",
    "    # ——————————propagation ————————\n",
    "    if f_minus == 0:\n",
    "        sentiment_recognition_both.append(1)\n",
    "    elif f_plus == 0:\n",
    "        sentiment_recognition_both.append(0)\n",
    "    elif abs(f_plus / f_minus) > lam:\n",
    "        sentiment_recognition_both.append(1)\n",
    "    elif abs(f_minus / f_plus) > lam:\n",
    "        sentiment_recognition_both.append(0)\n",
    "np.save('./NLP/SentimentAnalysis_2021_6/dataset/Amazon/result/lexicon_semi/sentiment_recognition_either.npy',\n",
    "        sentiment_recognition_either)\n",
    "np.save('./NLP/SentimentAnalysis_2021_6/dataset/Amazon/result/lexicon_semi/sentiment_recognition_both.npy',\n",
    "        sentiment_recognition_both)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}